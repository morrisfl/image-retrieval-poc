{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from annoy import AnnoyIndex\nfrom torchvision import transforms\n\nfrom src.data import Paris6kDataset\nfrom src.models import CLIPModelPretrained\nfrom src.utils import get_indexes, visualize_retrieval_results","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLIPModelPretrained(model_name=\"ViT-B/32\", pretrained_model=\"laion2b_s34b_b79k\")","metadata":{},"execution_count":null,"outputs":[]}]}